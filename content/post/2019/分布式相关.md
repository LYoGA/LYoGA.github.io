---
title: "分布式相关"
date: 2019-12-25T23:59:31+08:00
lastmod: 2019-12-25T23:59:31+08:00
draft: false
keywords:
-
description: ""
tags:
-
categories:
-
author: "lyoga"

---

<!--more-->
# **一、分布式缓存对比**
![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20191228200710.png)
![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20191228200222.png)

&nbsp;

## **1.1 缓存更新策略**
先更新数据库，后删除缓存，建议设置缓存有效时间，或者异步消息队列，或者binlog删除缓存

&nbsp;

# **二、分布式唯一ID**
## **2.1 UUID**
**UUID(Universally Unique Identifier)的标准型式包含32个16进制数字，以连字号分为五段，形式为8-4-4-4-12的36个字符，示例：550e8400-e29b-41d4-a716-446655440000**

### **1. 优点**

- 性能非常高：本地生成，没有网络消耗

&nbsp;

### **2. 缺点**

- 不易于存储：UUID太长，16字节128位，通常以36长度的字符串表示，很多场景不适用。
- 信息不安全：基于MAC地址生成UUID的算法可能会造成MAC地址泄露，这个漏洞曾被用于寻找梅丽莎病毒的制作者位置。
- ID作为主键时在特定的环境会存在一些问题，比如做DB主键的场景下，UUID就非常不适用：

&nbsp;

## **2.2 数据库**
**以MySQL举例，利用给字段设置auto_increment_increment和auto_increment_offset来保证ID自增，每次业务使用下列SQL读写MySQL得到ID号**

### **1. 优点**

- 非常简单，利用现有数据库系统的功能实现，成本小，有DBA专业维护
- ID号单调自增，可以实现一些对ID有特殊要求的业务

&nbsp;

### **2. 缺点**

- 强依赖DB，当DB异常时整个系统不可用，属于致命问题。配置主从复制可以尽可能的增加可用性，但是数据一致性在特殊情况下难以保证。主从切换时的不一致可能会导致重复发号
- ID发号性能瓶颈限制在单台MySQL的读写性能

&nbsp;

## **2.3 Redis**
**主要依赖于Redis是单线程的，所以也可以用生成全局唯一的ID。可以用Redis的原子操作 INCR 和 INCRBY 来实现**

### **1. 优点**

- 不依赖于数据库，灵活方便，且性能优于数据库
- 数字ID天然排序，对分页或者需要排序的结果很有帮助

&nbsp;

### **2. 缺点**

- 如果系统中没有Redis，还需要引入新的组件，增加系统复杂度。
- 需要编码和配置的工作量比较大

&nbsp;

## **2.4 Zookeeper**

- Zookeeper主要通过其znode数据版本来生成序列号，可以生成32位和64位的数据版本号，客户端可以使用这个版本号来作为唯一的序列号
- 很少会使用zookeeper来生成唯一ID。主要是由于需要依赖zookeeper，并且是多步调用API，如果在竞争较大的情况下，需要考虑使用分布式锁。因此，性能在高并发的分布式环境下，也不甚理想

&nbsp;

## **2.5 Leaf-segment数据库方案**
**Leaf-segment方案，在使用数据库的方案上，做了如下改变：**

- 原方案每次获取ID都得读写一次数据库，造成数据库压力大，改为利用proxy server批量获取，每次获取一个segment(step决定大小)号段的值，用完之后再去数据库获取新的号段，可以大大的减轻数据库的压力
- 各个业务不同的发号需求用biz_tag字段来区分，每个biz-tag的ID获取相互隔离，互不影响
- 如果以后有性能需求需要对数据库扩容，只需要对biz_tag分库分表就行

![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20191228210214.png)

&nbsp;

- biz_tag用来区分业务
- max_id表示该biz_tag目前所被分配的ID号段的最大值
- step表示每次分配的号段长度
- 原来获取ID每次都需要写数据库，现在只需要把step设置得足够大，比如1000。那么只有当1000个号被消耗完了之后才会去重新读写一次数据库。读写数据库的频率从1减小到了1/step

![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20191228210410.png)

&nbsp;

### **1. 优点**

- Leaf服务可以很方便的线性扩展，性能完全能够支撑大多数业务场景
- ID号码是趋势递增的8byte的64位数字，满足上述数据库存储的主键要求
- 容灾性高：Leaf服务内部有号段缓存，即使DB宕机，短时间内Leaf仍能正常对外提供服务
- 可以自定义max_id的大小，非常方便业务从原有的ID方式上迁移过来

&nbsp;

### **2. 缺点**

- ID号码不够随机，能够泄露发号数量的信息，不太安全
- TP999数据波动大，当号段使用完之后还是会hang在更新数据库的I/O上，tg999数据会出现偶尔的尖刺
- DB宕机会造成整个系统不可用

&nbsp;

### **2. 双buffer优化**
**问题：Leaf 取号段的时机是在号段消耗完的时候进行的，也就意味着号段临界点的ID下发时间取决于下一次从DB取回号段的时间，并且在这期间进来的请求也会因为DB号段没有取回来，导致线程阻塞。如果请求DB的网络和DB的性能稳定，这种情况对系统的影响是不大的，但是假如取DB的时候网络发生抖动，或者DB发生慢查询就会导致整个系统的响应时间变慢。**

&nbsp;

![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20191228210606.png)
**采用双buffer的方式，Leaf服务内部有两个号段缓存区segment。当前号段已下发10%时，如果下一个号段未更新，则另启一个更新线程去更新下一个号段。当前号段全部下发完后，如果下个号段准备好了则切换到下个号段为当前segment接着下发，循环往复。**

- 每个biz-tag都有消费速度监控，通常推荐segment长度设置为服务高峰期发号QPS的600倍（10分钟），这样即使DB宕机，Leaf仍能持续发号10-20分钟不受影响
- 每次请求来临时都会判断下个号段的状态，从而更新此号段，所以偶尔的网络抖动不会影响下个号段的更新

&nbsp;

**Leaf-segment方案可以生成趋势递增的ID，同时ID号是可计算的，不适用于订单ID生成场景，比如竞对在两天中午12点分别下单，通过订单id号相减就能大致计算出公司一天的订单量，这个是不能忍受的**

## **2.6 雪花算法-Snowflake**
![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20191228203712.png)

- 1bit：一般是符号位，不做处理
- 41bit：用来记录时间戳，这里可以记录69年，如果设置好起始时间比如今年是2018年，那么可以用到2089年，到时候怎么办？要是这个系统能用69年，我相信这个系统早都重构了好多次了。
- 10bit：10bit用来记录机器ID，总共可以记录1024台机器，一般用前5位代表数据中心，后面5位是某个数据中心的机器ID
- 12bit：循环位，用来对同一个毫秒之内产生不同的ID，12位可以最多记录4095个，也就是在同一个机器同一毫秒最多记录4095个，多余的需要进行等待下毫秒

&nbsp;

### **1. 优点**

- 毫秒数在高位，自增序列在低位，整个ID都是趋势递增的
- 不依赖数据库等第三方系统，以服务的方式部署，稳定性更高，生成ID的性能也是非常高的
- 可以根据自身业务特性分配bit位，非常灵活

&nbsp;

### **2. 缺点**

- 强依赖机器时钟，如果机器上时钟回拨，会导致发号重复或者服务会处于不可用状态

&nbsp;

## **2.7 Leaf-snowflake方案**

- Leaf-snowflake方案完全沿用snowflake方案的bit位设计，即是“1+41+10+12”的方式组装ID号
- 对于workerID的分配，当服务集群数量较小的情况下，完全可以手动配置;Leaf服务规模较大，动手配置成本太高。所以使用Zookeeper持久顺序节点的特性自动对snowflake节点配置wokerID

&nbsp;

**Leaf-snowflake是按照下面几个步骤启动的：**

- 启动Leaf-snowflake服务，连接Zookeeper，在leaf_forever父节点下检查自己是否已经注册过（是否有该顺序子节点）
- 如果有注册过直接取回自己的workerID（zk顺序节点生成的int类型ID号），启动服务
- 如果没有注册过，就在该父节点下面创建一个持久顺序节点，创建成功后取回顺序号当做自己的workerID号，启动服务

![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20191228211207.png)

&nbsp;

### **1. 弱依赖ZooKeeper**
**除了每次会去ZK拿数据以外，也会在本机文件系统上缓存一个workerID文件。当ZooKeeper出现问题，恰好机器出现问题需要重启时，能保证服务能够正常启动。这样做到了对三方组件的弱依赖。一定程度上提高了SLA**

&nbsp;

### **2. 防止时钟回拨**
![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20191228211416.png)
**参见上图整个启动流程图，服务启动时首先检查自己是否写过ZooKeeper leaf_forever节点：**

- 若写过，则用自身系统时间与leaf_forever/${self}节点记录时间做比较，若小于leaf_forever/${self}时间则认为机器时间发生了大步长回拨，服务启动失败并报警。
- 若未写过，证明是新服务节点，直接创建持久节点leaf_forever/${self}并写入自身系统时间，接下来综合对比其余Leaf节点的系统时间来判断自身系统时间是否准确，具体做法是取leaf_temporary下的所有临时节点(所有运行中的Leaf-snowflake节点)的服务IP：Port，然后通过RPC请求得到所有节点的系统时间，计算sum(time)/nodeSize。
- 若abs(系统时间-sum(time)/nodeSize) < 阈值，认为当前系统时间准确，正常启动服务，同时写临时节点leaf_temporary/${self} 维持租约
- 否则认为本机系统时间发生大步长偏移，启动失败并报警。
- 每隔一段时间(3s)上报自身系统时间写入leaf_forever/${self}

**由于强依赖时钟，对时间的要求比较敏感，在机器工作时NTP同步也会造成秒级别的回退，建议可以直接关闭NTP同步。要么在时钟回拨的时候直接不提供服务直接返回ERROR_CODE，等时钟追上即可。或者做一层重试，然后上报报警系统，更或者是发现有时钟回拨之后自动摘除本身节点并报警**

&nbsp;

## **参考**

- https://tech.meituan.com/2017/04/21/mt-leaf.html

&nbsp;

# **三、分布式锁**
## **3.1 为何需要分布式锁**

- 效率：使用分布式锁可以避免不同节点重复相同的工作，这些工作会浪费资源。比如用户付了钱之后有可能不同节点会发出多封短信
- 正确性：加分布式锁同样可以避免破坏正确性的发生，如果两个节点在同一条数据上面操作，比如多个节点机器对同一个订单操作不同的流程有可能会导致该笔订单最后状态出现错误，造成损失

&nbsp;

## **3.2 分布式锁的特点**

- 互斥性：和我们本地锁一样互斥性是最基本，但是分布式锁需要保证在不同节点的不同线程的互斥
- 可重入性：同一个节点上的同一个线程如果获取了锁之后那么也可以再次获取这个锁
- 锁超时：和本地锁一样支持锁超时，防止死锁
- 高效，高可用：加锁和解锁需要高效，同时也需要保证高可用防止分布式锁失效，可以增加降级
- 支持阻塞和非阻塞：和ReentrantLock一样支持lock和trylock以及tryLock(long timeOut)
- 支持公平锁和非公平锁(可选)：公平锁的意思是按照请求加锁的顺序获得锁，非公平锁就相反是无序的；这个一般来说实现的比较少

&nbsp;

## **3.3 数据库**
![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20191229154716.png)

![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20191229155022.png)

- lock：阻塞式获取，while循环lock方法
- trylock：非阻塞
- tryLock(time)：基于时间阻塞
- unlock：unlock的话如果这里的count为1那么可以删除，如果大于1那么需要减去1（基于事务）

**数据表可以加个version字段，基于乐观锁CAS实现**

&nbsp;

**问题及解决方案：**
- 性能问题数据库单点(主从)
- 解锁失败无失效时间(定时任务清理)
- 插入失败即报错非阻塞(加个while循环)
- 非重入（表中加机器及线程信息)

&nbsp;

## **3.4 Zookeeper**
![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20191229155305.png)
**/lock是我们用于加锁的目录,/resource_name是我们锁定的资源，其下面的节点按照我们加锁的顺序排列**

- 客户端尝试创建一个znode节点，比如/lock/resource_name；那么第一个客户端就创建成功了，相当于拿到了锁；而其它的客户端会创建失败（znode已存在），获取锁失败
- 持有锁的客户端访问共享资源完成后，将znode删掉，这样其它客户端接下来就能来获取锁了
- znode应该被创建成ephemeral的。这是znode的一个特性，它保证如果创建znode的那个客户端崩溃了，那么相应的znode会被自动删除。这保证了锁一定会被释放

&nbsp;

### **1. ZooKeeper是怎么检测出某个客户端已经崩溃了呢？**
实际上，每个客户端都与ZooKeeper的某台服务器维护着一个Session，这个Session依赖定期的心跳(heartbeat)来维持。如果ZooKeeper长时间收不到客户端的心跳（这个时间称为Sesion的过期时间），那么它就认为Session过期了，通过这个Session所创建的所有的ephemeral类型的znode节点都会被自动删除

&nbsp;

### **2. 针对Zookeeper羊群效应问题**

- 问题：羊群效应就是指这样，大量的client都会获得相同的通知，而只有一小部分会对事件通知有响应。而分布式锁，我们这里只有一个client获得锁，但是所有的client都获得了通知。这样就会给Zookeeper的服务器增加了很大的压力
- 解决：为了避免羊群效应，其实我们可以将通知的范围设置的更加精确，也就说只给那部分对事件通知有响应的client客户端发送通知。我们通过发现，只有排序在当前znode之前一个znode离开时，才有必要通知创建当前znode的client，而不必在任意一个znode或者删除时都通知client

**Curator实现了可重入锁(InterProcessMutex)，也实现了不可重入锁(InterProcessSemaphoreMutex)，并在可重入锁中还实现了读写锁。**

&nbsp;

## **3.5 Redis**
```
set resourceName value ex 5 nx
```

- 在Redis2.8之前我们需要使用Lua脚本达到我们的目的，但是redis2.8之后redis支持nx和ex操作是同一原子操作
- Redission也是Redis的客户端，相比于Jedis功能简单，Jedis简单使用阻塞的I/O和redis交互，Redission通过Netty支持非阻塞I/O

&nbsp;

## **3.6 RedLock**
**我们想象一个这样的场景当机器A申请到一把锁之后，如果Redis主宕机了，这个时候从机并没有同步到这一把锁，那么机器B再次申请的时候就会再次申请到这把锁，为了解决这个问题Redis作者提出了RedLock红锁的算法,在Redission中也对RedLock进行了实现**

&nbsp;

### **1. 加锁**

- 获取当前时间（毫秒数）
- 按顺序依次向N个Redis节点执行获取锁的操作。这个获取操作跟前面基于单Redis节点的获取锁的过程相同，包含随机字符串my_random_value，也包含过期时间(比如PX 30000，即锁的有效时间)。为了保证在某个Redis节点不可用的时候算法能够继续运行，这个获取锁的操作还有一个超时时间(time out)，它要远小于锁的有效时间（几十毫秒量级）。客户端在向某个Redis节点获取锁失败以后，应该立即尝试下一个Redis节点。这里的失败，应该包含任何类型的失败，比如该Redis节点不可用，或者该Redis节点上的锁已经被其它客户端持有（注：Redlock原文中这里只提到了Redis节点不可用的情况，但也应该包含其它的失败情况）
- 计算整个获取锁的过程总共消耗了多长时间，计算方法是用当前时间减去第1步记录的时间。如果客户端从大多数Redis节点（>= N/2+1）成功获取到了锁，并且获取锁总共消耗的时间没有超过锁的有效时间(lock validity time)，那么这时客户端才认为最终获取锁成功；否则，认为最终获取锁失败
- 如果最终获取锁成功了，那么这个锁的有效时间应该重新计算，它等于最初的锁的有效时间减去第3步计算出来的获取锁消耗的时间
- 如果最终获取锁失败了（可能由于获取到锁的Redis节点个数少于N/2+1，或者整个获取锁的过程消耗的时间超过了锁的最初有效时间），那么客户端应该立即向所有Redis节点发起释放锁的操作（即前面介绍的Redis Lua脚本）

**总结：顺序地在 所有实例上申请锁，使用相同的 key 和 random value，一半以上获取到锁 申请成功；申请失败释放锁；**

**问题：其中一个redis重启（同步持久化或者启动后的Redis一段时间不可用待锁过期）**

&nbsp;

### **2. 解锁**
**客户端向所有Redis节点发起释放锁的操作，不管这些节点当时在获取锁的时候成功与否，原因是可能已写入节点，response相应超时**

&nbsp;

### **3. 问题**
- GC的STW
- 时钟发生跳跃
- 长时间网络IO

&nbsp;

## **比较**

- 从理解的难易程度角度（从低到高）
  - 数据库 > 缓存 > Zookeeper
- 从实现的复杂性角度（从低到高）
  - Zookeeper >= 缓存 > 数据库
- 从性能角度（从高到低）
  - 缓存 > Zookeeper >= 数据库
- 从可靠性角度（从高到低）
- Zookeeper > 缓存 > 数据库

&nbsp;

## **参考**

- https://mp.weixin.qq.com/s?__biz=MzA4NTg1MjM0Mg==&mid=2657261514&idx=1&sn=47b1a63f065347943341910dddbb785d&chksm=84479e13b3301705ea29c86f457ad74010eba8a8a5c12a7f54bcf264a4a8c9d6adecbe32ad0b&mpshare=1&scene=23&srcid=1004XqCveZ8C5IDjoB9ZXbWj#rd
- https://juejin.im/post/5bbb0d8df265da0abd3533a5

&nbsp;

# **四、分布式事务**
## **4.1 CAP**

- C (一致性)：对某个指定的客户端来说，读操作能返回最新的写操作。对于数据分布在不同节点上的数据上来说，如果在某个节点更新了数据，那么在其他节点如果都能读取到这个最新的数据，那么就称为强一致，如果有某个节点没有读取到，那就是分布式不一致
- A (可用性)：非故障的节点在合理的时间内返回合理的响应(不是错误和超时的响应)。可用性的两个关键一个是合理的时间，一个是合理的响应。合理的时间指的是请求不能无限被阻塞，应该在合理的时间给出返回。合理的响应指的是系统应该明确返回结果并且结果是正确的，这里的正确指的是比如应该返回50，而不是返回40
- P (分区容错性)：当出现网络分区后，系统能够继续工作。打个比方，这里个集群有多台机器，有台机器网络出现了问题，但是这个集群仍然可以正常工作

**分布式系统理论上不可能选择CA架构，只能选择CP或者AP架构**

&nbsp;

## **4.2 BASE**
**BASE 是 Basically Available(基本可用)、Soft state(软状态)和 Eventually consistent (最终一致性)三个短语的缩写，是对CAP中AP的一个扩展**

- 基本可用：分布式系统在出现故障时，允许损失部分可用功能，保证核心功能可用
- 软状态：允许系统中存在中间状态，这个状态不影响系统可用性，这里指的是CAP中的不一致
- 最终一致：最终一致是指经过一段时间后，所有节点数据都将会达到一致

**BASE解决了CAP中理论没有网络延迟，在BASE中用软状态和最终一致，保证了延迟后的一致性。BASE 和 ACID 是相反的，它完全不同于ACID的强一致性模型，而是通过牺牲强一致性来获得可用性，并允许数据在一段时间内是不一致的，但最终达到一致状态。**

&nbsp;

## **4.3 2PC**
![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20191229205739.png)

- 单点问题：事务管理器在整个流程中扮演的角色很关键，如果其宕机，比如在第一阶段已经完成，在第二阶段正准备提交的时候事务管理器宕机，资源管理器就会一直阻塞，导致数据库无法使用
- 同步阻塞：在准备就绪之后，资源管理器中的资源一直处于阻塞，直到提交完成，释放资源
- 数据不一致：两阶段提交协议虽然为分布式数据强一致性所设计，但仍然存在数据不一致性的可能，比如在第二阶段中，假设协调者发出了事务commit的通知，但是因为网络问题该通知仅被一部分参与者所收到并执行了commit操作，其余的参与者则因为没有收到通知一直处于阻塞状态，这时候就产生了数据的不一致性

&nbsp;

## **4.4 3PC**
![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20200104151736.png)

**协调者、参与者都引入了超时机制；三阶段 CanCommit、PreCommit（其中一个超时或者执行失败，则TM发起中断）和doCommit**

- 优点 ：引入超时机制，降低了协调者与参与者之间的阻塞范围   
- 缺点：在参与者接收到preCommit之后，如果出现网络分区，那么该参与者节点会继续执行事务的提交，而其他节点会执行中断事务，最终会造成数据的不一致

&nbsp;

## **4.5 TCC**

- Try阶段： 完成所有业务检查（一致性），预留业务资源(准隔离性)
- Confirm阶段：确认执行业务操作，不做任何业务检查， 只使用Try阶段预留的业务资源。
- Cancel阶段：  取消Try阶段预留的业务资源。

## **4.6 本地消息表**

## **4.7 MQ事务**
**在RocketMQ中实现了分布式事务，实际上其实是对本地消息表的一个封装，将本地消息表移动到了MQ内部**

&nbsp;

## **4.8 Saga事务**
**其核心思想是将长事务拆分为多个本地短事务，由Saga事务协调器协调，如果正常结束那就正常完成，如果某个步骤失败，则根据相反顺序一次调用补偿操作。**

&nbsp;

# **五、分布式一致性算法**
## **5.1 Paxos**
https://www.cnblogs.com/linbingdong/p/6253479.html

&nbsp;

### **1. 在Paxos算法中有三种角色**

- Proposer（提议者）：只要Proposer发的提案被Acceptor接受（刚开始先认为只需要一个Acceptor接受即可，在推导过程中会发现需要半数以上的Acceptor同意才行），Proposer就认为该提案里的value被选定了
- Acceptor（接收者）：只要Acceptor接受了某个提案，Acceptor就认为该提案里的value被选定了
- Learner（学习者）：Acceptor告诉Learner哪个value被选定，Learner就认为那个value被选定

**一个进程可能同时充当多种角色。比如一个进程可能既是Proposer又是Acceptor又是Learner。**

&nbsp;

### **2. Paxos算法分为两个阶段**

- 阶段一：
  - (a) Proposer选择一个提案编号N，然后向半数以上的Acceptor发送编号为N的Prepare请求
  - (b) 如果一个Acceptor收到一个编号为N的Prepare请求，且N大于该Acceptor已经响应过的所有Prepare请求的编号，那么它就会将它已经接受过的编号最大的提案（如果有的话）作为响应反馈给Proposer，同时该Acceptor承诺不再接受任何编号小于N的提案
- 阶段二：
  - (a) 如果Proposer收到半数以上Acceptor对其发出的编号为N的Prepare请求的响应，那么它就会发送一个针对[N,V]提案的Accept请求给半数以上的Acceptor。注意：V就是收到的响应中编号最大的提案的value，如果响应中不包含任何提案，那么V就由Proposer自己决定
  - (b) 如果Acceptor收到一个针对编号为N的提案的Accept请求，只要该Acceptor没有对编号大于N的Prepare请求做出过响应，它就接受该提案

&nbsp;

### **3. Learner学习被选定的value**
![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20200104170633.png)

&nbsp;

### **4. 如何保证Paxos算法的活性**
![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20200104170705.png)

&nbsp;

## **5.2 Zab**
**Zookeeper的核心是原子广播机制，这个机制保证了各个server之间的同步。实现这个机制的协议叫做Zab协议。Zab协议有两种模式，它们分别是恢复模式和广播模式。**

![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20200105153913.png)

- Leader：一个ZooKeeper集群同一时间只会有一个实际工作的Leader，它会发起并维护与各Follwer及Observer间的心跳。所有的写操作必须要通过Leader完成再由Leader将写操作广播给其它服务器
- Follower：一个ZooKeeper集群可能同时存在多个Follower，它会响应Leader的心跳。Follower可直接处理并返回客户端的读请求，同时会将写请求转发给Leader处理，并且负责在Leader处理写请求时对请求进行投票
- Observer：角色与Follower类似，但是无投票权

&nbsp;

### **1. 消息广播**

![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20200105154137.png)

- Leader 接收到消息请求后，将消息赋予一个全局唯一的 64 位自增 id，叫做：zxid，通过 zxid 的大小比较即可实现因果有序这一特性
- Leader 通过先进先出队列（会给每个follower都创建一个队列，保证发送的顺序性）（通过 TCP 协议来实现，以此实现了全局有序这一特性）将带有 zxid 的消息作为一个提案（proposal）分发给所有 follower
- 当 follower 接收到 proposal，先将 proposal 写到本地事务日志，写事务成功后再向 leader 回一个 ACK
- 当 leader 接收到过半的 ACKs 后，leader 就向所有 follower 发送 COMMIT 命令，同意会在本地执行该消息
- 当 follower 收到消息的 COMMIT 命令时，就会执行该消息

&nbsp;

### **2. 细节问题**

- Leader 在收到客户端请求之后，会将这个请求封装成一个事务，并给这个事务分配一个全局递增的唯一 ID，称为事务ID（ZXID），ZAB 协议需要保证事务的顺序，因此必须将每一个事务按照 ZXID 进行先后排序然后处理。
- 在 Leader 和 Follwer 之间还有一个消息队列，用来解耦他们之间的耦合，解除同步阻塞。
- zookeeper集群中为保证任何所有进程能够有序的顺序执行，只能是 Leader 服务器接受写请求，即使是 Follower 服务器接受到客户端的请求，也会转发到 Leader 服务器进行处理。
- 实际上，这是一种简化版本的 2PC，不能解决单点问题。等会我们会讲述 ZAB 如何解决单点问题（即 Leader 崩溃问题）。

&nbsp;

### **3. 崩溃恢复**
**ZAB协议会让ZK集群进入崩溃恢复模式的情况如下：**

- 当服务框架在启动过程中
- 当Leader服务器出现网络中断，崩溃退出与重启等异常情况
- 当集群中已经不存在过半的服务器与Leader服务器保持正常通信

**Leader选举算法：基于TCP的FastLeaderElection**

- 先判断logicClock是否为同一轮选举，之后选票PK先比Zxid，相同再比myid，myid大优先作为leader服务

&nbsp;

### **4. 选主需解决问题**

- 已经被leader提交的事务需要最终被所有的机器提交（已经发出commit了）
- 保证丢弃那些只在leader上提出的事务（只在leader上提出了proposal，还没有收到回应，还没有进行提交）

**已经被处理的消息不能丢（commit的）**

- 选举拥有 proposal 最大值（即 zxid 最大）的节点作为新的 leader：由于所有提案被 COMMIT 之前必须有合法数量的 follower ACK，即必须有合法数量的服务器的事务日志上有该提案的 proposal，因此，只要有合法数量的节点正常工作，就必然有一个节点保存了所有被 COMMIT 消息的 proposal 状态
- 新的 leader 将自己事务日志中 proposal 但未 COMMIT 的消息处理
- 新的 leader 与 follower 建立先进先出的队列， 先将自身有而 follower 没有的 proposal 发送给 follower，再将这些 proposal 的 COMMIT 命令发送给 follower，以保证所有的 follower 都保存了所有的 proposal、所有的 follower 都处理了所有的消息
- 通过以上策略，能保证已经被处理的消息不会丢

**被丢弃的消息不能再次出现**

- Zab 通过巧妙的设计 zxid 来实现这一目的
- zxid 是64位，高 32 是纪元（epoch）编号，每经过一次 leader 选举产生一个新的 leader，新 leader 会将 epoch 号 +1。
- 低 32 位是消息计数器，每接收到一条消息这个值 +1，新 leader 选举后这个值重置为 0
- 这样设计的好处是旧的 leader 挂了后重启，它不会被选举为 leader，因为此时它的 zxid 肯定小于当前的新 leader；当旧的 leader 作为 follower 接入新的 leader 后，新的 leader 会让它将所有的拥有旧的 epoch 号的未被 COMMIT 的 proposal 清除

&nbsp;

### **参考**

- https://dbaplus.cn/news-141-1875-1.html
- https://www.cnblogs.com/stateis0/p/9062133.html
- https://blog.haobin95.club/2019/01/03/%E4%B8%AD%E9%97%B4%E4%BB%B6/zookeeper%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%B0%83%E8%AF%A6%E8%A7%A3/

&nbsp;

## **5.3 Raft**
**Raft 算法是一种简单易懂的共识算法。它依靠 状态机 和 主从同步 的方式，在各个节点之间实现数据的一致性。**

**在一个由 Raft 协议组织的集群中有三类角色：**

- Leader（领袖）
- Follower（群众）
- Candidate（候选人）

&nbsp;

### **1. 选主**

1. 在最初，还没有一个主节点的时候，所有节点的身份都是Follower。每一个节点都有自己的计时器，当计时达到了超时时间（Election Timeout），该节点会转变为Candidate
2. 成为Candidate的节点，会首先给自己投票，然后向集群中其他所有的节点发起请求，要求大家都给自己投票
3. 其他收到投票请求且还未投票的Follower节点会向发起者投票，发起者收到反馈通知后，票数增加
4. 当得票数超过了集群节点数量的一半，该节点晋升为Leader节点。Leader节点会立刻向其他节点发出通知，告诉大家自己才是老大。收到通知的节点全部变为Follower，并且各自的计时器清零

**这里需要说明一点，每个节点的超时时间都是不一样的。比如A节点的超时时间是3秒，B节点的超时时间是5秒，C节点的超时时间是4秒。这样一来，A节点将会最先发起投票请求，而不是所有节点同时发起。**

- 为什么这样设计呢？设想如果所有节点同时发起投票，必然会导致大家的票数差不多，形成僵局，谁也当不成老大。

**那么，成为Leader的节点是否就坐稳了老大的位置呢？**

- 并不是。Leader节点需要每隔一段时间向集群其他节点发送心跳通知，表明你们的老大还活着。
- 一旦Leader节点挂掉，发不出通知，那么计时达到了超时时间的Follower节点会转变为Candidate节点，发起选主投票，周而复始......

&nbsp;

### **2. 数据同步**

- Raft 协议强依赖 Leader 节点的可用性来确保集群数据的一致性；数据的流向只能从 Leader 节点向 Follower 节点转移
- 由客户端提交数据到Leader节点
- 由Leader节点把数据复制到集群内所有的Follower节点。如果一次复制失败，会不断进行重试
- Follower节点们接收到复制的数据，会反馈给Leader节点
- 如果Leader节点接收到超过半数的Follower反馈，表明复制成功。于是提交自己的数据，并通知客户端数据提交成功
- 由Leader节点通知集群内所有的Follower节点提交数据，从而完成数据同步流程

&nbsp;

### **3. 脑裂问题**
网络分区将原先的 Leader 节点和 Follower 节点分隔开，Follower 收不到 Leader 的心跳将发起选举产生新的 Leader。这时就产生了双 Leader，原先的 Leader 独自在一个区，向它提交数据不可能复制到多数节点所以永远提交不成功。向新的 Leader 提交数据可以提交成功，网络恢复后旧的 Leader 发现集群中有更新任期（Term）的新 Leader 则自动降级为 Follower 并从新 Leader 处同步数据达成集群数据一致

&nbsp;

### **参考**

- https://www.cnblogs.com/mindwind/p/5231986.html
