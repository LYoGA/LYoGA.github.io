---
title: "并发编程-总结"
date: 2019-12-17T23:29:25+08:00
lastmod: 2019-12-17T23:29:25+08:00
draft: false 
keywords:
-
description: ""
tags:
-
categories:
-
author: "lyoga"

---

<!--more-->
# **一、线程**
## **1.1 线程状态**
![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20191218174154.png)
![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20191218171738.png)

- 当线程进入到synchronized方法或者synchronized代码块无法获取锁时，线程切换到的是BLOCKED状态
- 而使用java.util.concurrent.locks下lock进行加锁的时候线程切换的是WAITING或者TIMED_WAITING状态，因为lock会调用LockSupport的方法

&nbsp;

## **1.2 新建线程**
- 继承Thread类，重写run方法
- 实现runable接口
- 实现callable接口

```
public static void main(String[] args) {
        //1.继承Thread
        Thread thread = new Thread() {
            @Override
            public void run() {
                System.out.println("继承Thread");
                super.run();
            }
        };
        thread.start();
        //2.实现runable接口
        Thread thread1 = new Thread(new Runnable() {
            @Override
            public void run() {
                System.out.println("实现runable接口");
            }
        });
        thread1.start();
        //3.实现callable接口
        ExecutorService service = Executors.newSingleThreadExecutor();
        Future<String> future = service.submit(new Callable() {
            @Override
            public String call() throws Exception {
                return "通过实现Callable接口";
            }
        });
        try {
            String result = future.get();
            System.out.println(result);
        } catch (InterruptedException e) {
            e.printStackTrace();
        } catch (ExecutionException e) {
            e.printStackTrace();
        }
    }
```
![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20191218204705.png)

&nbsp;

## **1.3 线程状态的基本操作**

### **1.3.1 interrupted**
![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20191218204751.png)

- 中断可以理解为线程的一个标志位，它表示了一个运行中的线程是否被其他线程进行了中断操作
- 中断好比其他线程对该线程打了一个招呼，其他线程可以调用该线程的interrupt()方法对其进行中断操作，同时该线程可以调用 isInterrupted（）来感知其他线程对其自身的中断操作，从而做出响应
- 同样可以调用Thread的静态方法interrupted（）对当前线程进行中断操作，该方法会清除中断标志位
- 需要注意的是，当抛出InterruptedException时候，会清除中断标志位，也就是说在调用isInterrupted会返回false

```
public static void main(String[] args) throws InterruptedException {
        //sleepThread睡眠1000ms
        final Thread sleepThread = new Thread() {
            @Override
            public void run() {
                try {
                    Thread.sleep(1000);
                } catch (InterruptedException e) {
                    e.printStackTrace();
                }
            }
        };
        //busyThread一直执行死循环
        Thread busyThread = new Thread() {
            @Override
            public void run() {
                while (true) ;
            }
        };
        sleepThread.start();
        busyThread.start();
        sleepThread.interrupt();
        busyThread.interrupt();
        while (sleepThread.isInterrupted()) ;
        System.out.println("sleepThread isInterrupted: " + sleepThread.isInterrupted());
        System.out.println("busyThread isInterrupted: " + busyThread.isInterrupted());
    }
```
- sleepThread抛出InterruptedException(sleep方法)后清除标志位，而busyThread就不会清除标志位
- while (sleepThread.isInterrupted()) 表示在Main中会持续监测sleepThread，一旦sleepThread的中断标志位清零，即sleepThread.isInterrupted()返回为false时才会继续Main线程才会继续往下执行
- 结束线程时通过中断标志位或者标志位的方式可以有机会去清理资源，相对于武断而直接的结束线程，这种方式要优雅和安全

&nbsp;

### **1.3.2 join**
- join方法可以看做是线程间协作的一种方式，很多时候，一个线程的输入可能非常依赖于另一个线程的输出，这就像两个好基友，一个基友先走在前面突然看见另一个基友落在后面了，这个时候他就会在原处等一等这个基友，等基友赶上来后，就两人携手并进
- Thread类除了提供join()方法外，另外还提供了超时等待的方法，如果线程threadB在等待的时间内还没有结束的话，threadA会在超时之后继续执行

```
while (isAlive()) {
    wait(0);
 }
```

**可以看出来当前等待对象threadA会一直阻塞，直到被等待对象threadB结束后即isAlive()返回false的时候才会结束while循环，当threadB退出时会调用notifyAll()方法通知所有的等待线程**

&nbsp;

### **1.3.3 sleep**
```
public static native void sleep(long millis)
```
**这是Thread的静态方法，很显然它是让当前线程按照指定的时间休眠，其休眠时间的精度取决于处理器的计时器和调度器。需要注意的是如果当前线程获得了锁，sleep方法并不会失去锁。sleep方法经常拿来与Object.wait()方法进行比价，这也是面试经常被问的地方**


**sleep() VS wait()**

- sleep()方法是Thread的静态方法，而wait是Object实例方法
- wait()方法必须要在同步方法或者同步块中调用，也就是必须已经获得对象锁；而sleep()方法没有这个限制可以在任何地方种使用。
- wait()方法会释放占有的对象锁，使得该线程进入等待池中，等待下一次获取资源；而sleep()方法只是会让出CPU并不会释放掉对象锁；
- sleep()方法在休眠时间达到后如果再次获得CPU时间片就会继续执行，而wait()方法必须等待Object.notift/Object.notifyAll通知后，才会离开等待池，并且再次获得CPU时间片才会继续执行

&nbsp;

### **1.3.4 yield**
```
public static native void yield()**
```
**这是一个静态方法，一旦执行，它会是当前线程让出CPU，但是，需要注意的是，让出的CPU并不是代表当前线程不再运行了，如果在下一次竞争中，又获得了CPU时间片当前线程依然会继续运行。另外，让出的时间片只会分配给当前线程相同优先级的线程**

在Java程序中，通过一个整型成员变量Priority来控制优先级，优先级的范围从1~10.在构建线程的时候可以通过setPriority(int)方法进行设置，默认优先级为5，优先级高的线程相较于优先级低的线程优先获得处理器时间片。需要注意的是在不同JVM以及操作系统上，线程规划存在差异，有些操作系统甚至会忽略线程优先级的设定。

- sleep()：交出来的时间片其他线程都可以去竞争
- yield()：只允许与当前线程具有相同优先级的线程能够获得释放出来的CPU时间片

&nbsp;

## **1.4 守护线程Daemon**
- 守护线程是一种特殊的线程，就和它的名字一样，它是系统的守护者，在后台默默地守护一些系统服务，比如垃圾回收线程，JIT线程就可以理解守护线程
- 与之对应的就是用户线程，用户线程就可以认为是系统的工作线程，它会完成整个系统的业务操作
- 用户线程完全结束后就意味着整个系统的业务任务全部结束了，因此系统就没有对象需要守护的了，守护线程自然而然就会退
- 当一个Java应用，只有守护线程的时候，虚拟机就会自然退出

```
public static void main(String[] args) {
    Thread daemonThread = new Thread(new Runnable() {
        @Override
        public void run() {
            while (true) {
                try {
                    System.out.println("i am alive");
                    Thread.sleep(500);
                } catch (InterruptedException e) {
                    e.printStackTrace();
                } finally {
                    System.out.println("finally block");
                }
            }
        }
    });
    daemonThread.setDaemon(true);
    daemonThread.start();
    //确保main线程结束前能给daemonThread能够分到时间片
    try {
        Thread.sleep(800);
    } catch (InterruptedException e) {
        e.printStackTrace();
    }
}
```
**通过daemonThread.setDaemon(true)，讲daemonThread设置成守护线程，所以当main线程结束后，守护线程daemon自然也退出了，这边sleep 8s是为了给daemonThead线程有足够时间打印finally块里面的东西，因为守护线程在退出的时候并不会执行finnaly块中的代码，所以将释放资源等操作不要放在finnaly块中执行，这种操作是不安全的**

&nbsp;

# **二、内存模型**
https://lyoga.github.io/post/2018/12-03-jmm/

- 线程之间的共享变量存储在主内存(main memory)中，每个线程都有一个私有的本地内存(local memory)，本地内存中存储了该线程以读/写共享变量的副本
- 不同线程通过主内存进行通信
- 主内存映射到cpu的缓存系统、内存
- 本地内存映射到cpu私有的数据结构，如寄存器、与cache之间的指令buffer

**JMM通过控制主内存与每个线程的本地内存之间的交互，为java程序员提供内存可见性保证**

&nbsp;

# **三、volatile**

- 可见性：对一个 volatile变量的读，总是能看到(任意线程)对这个volatile变量最后的写入
- 原子性：对任意单个volatile变量的读/写具有原子性，但类似于volatile++这种复合操作不具有原子性
- 有序性：禁止重排序（包括编译器重排和处理器重排）

**为了实现volatile的内存语义，编译器在生成字节码时，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序**

&nbsp;

# **四、synchronized**

## **4.1 特性**

- 可见性
- 原子性
- 有序性

&nbsp;

## **4.2 使用方法**

- synchronized关键字：通过monitorenter（入锁）和monitorexit（出锁）两个指令实现的
- synchronized方法：方法的flags里有ACC_SYNCHRONIZED

&nbsp;

## **4.3 背景**

1. 在JDK1.6之前，synchronized属于重量级锁，效率低下，因为监视器锁(monitor)是依赖于底层的操作系统互斥Mutex Lock来实现的，而操作系统实现线程之间的阻塞、调度、唤醒等操作时需要从用户态切换到内核态，最后再由内核态切换到用户态，将CPU的控制权交由用户进程，用户态与内核态之间频繁的切换，严重影响锁的性能，这也是为什么早期的synchronized效率低的原因
2. 在JDK1.6引入了两种新型锁机制：偏向锁和轻量级锁，它们的引入是为了解决在没有多线程竞争或基本没有竞争的场景下因使用传统锁机制带来的性能开销问题

**无锁->偏向锁->轻量级锁->重量级锁**

![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20191218210203.png)

&nbsp;

## **4.4 MarkWord**
![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20191218210341.png)

&nbsp;

## **4.5 轻量级锁**
![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20191222162213.png)
![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20191222162239.png)
![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20191222162310.png)

&nbsp;

## **4.6 偏向锁**
![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20191222162341.png)
![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20191222162412.png)
![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20191222162433.png)
![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20191222162501.png)
![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20191222162523.png)

&nbsp;

**如何实现批量重偏向&撤销**

- 以class为单位，为每个class维护一个偏向锁撤销计数器，每一次该class的对象发生偏向撤销操作时，该计数器+1，当这个值达到重偏向阈值（默认20）时，JVM就认为该class的偏向锁有问题，因此会进行批量重偏向
- 每个class对象会有一个对应的epoch字段，每个处于偏向锁状态对象的mark word中也有该字段，其初始值为创建该对象时，class中的epoch的值。每次发生批量重偏向时，就将该值+1，同时遍历JVM中所有线程的栈，找到该class所有正处于加锁状态的偏向锁，将其epoch字段改为新值
- 下次获得锁时，发现当前对象的epoch值和class的epoch不相等，那就算当前已经偏向了其他线程，也不会执行撤销操作，而是直接通过CAS操作将其mark word的Thread Id 改成当前线程Id
- 当达到重偏向阈值后，假设该class计数器继续增长，当其达到批量撤销的阈值后（默认40），JVM就认为该class的使用场景存在多线程竞争，会标记该class为不可偏向，之后，对于该class的锁，直接走轻量级锁的逻辑

&nbsp;

## **4.7 重量级锁**
**重量级锁是通过对象内部的一个叫做监视器锁（monitor）来实现的，监视器锁本质又是依赖于底层的操作系统的Mutex Lock（互斥锁）来实现的。而操作系统实现线程之间的切换需要从用户态转换到内核态，这个成本非常高，状态之间的转换需要相对比较长的时间，这就是为什么synchronized效率低的原因。**

![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20191222162714.png)

1. 当一个线程尝试获得锁时，如果该锁已经被占用，则会将该线程封装成一个ObjectWaiter对象插入到cxq的队列的队首，然后调用park函数挂起当前线程
2. 当线程释放锁时，会从cxq或EntryList中挑选一个线程唤醒，被选中的线程叫做Heir presumptive即假定继承人，就是图中的Ready Thread，假定继承人被唤醒后会尝试获得锁，但synchronized是非公平的，所以假定继承人不一定能获得锁（这也是它叫"假定"继承人的原因）
3. 如果线程获得锁后调用Object#wait方法，则会将线程加入到WaitSet中，当被Object#notify唤醒后，会将线程从WaitSet移动到cxq或EntryList中去。需要注意的是，当调用一个锁对象的wait或notify方法时，如当前锁的状态是偏向锁或轻量级锁则会先膨胀成重量级锁

![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20191222162809.png)

&nbsp;

- 偏向锁锁解决的是一个周期内“单线程”访问共享资源问题，连CAS操作都是能节省就尽量节省
- 轻量级锁解决的是一个周期内多线程交替访问共享资源问题，使用CAS操作消除底层系统的互斥
- 重量级锁解决的是一个周期内同时访问共享资源问题，需要管理等待线程以及依赖于底层系统互斥指令

![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20191222162838.png)

&nbsp;

## **4.8 其他优化**

- 自适应自旋锁
- 锁消除：例如for循环
- 锁粗化：例如StringBuffer

&nbsp;

## **4.9 Synchronized和ReentrantLock的区别**
- Synchronized是JVM层次的锁实现，ReentrantLock是JDK层次的锁实现；
- Synchronized的锁状态是无法在代码中直接判断的，但是ReentrantLock可以通过ReentrantLock#isLocked判断；
- Synchronized是非公平锁，ReentrantLock是可以是公平也可以是非公平的；
- Synchronized是不可以被中断的，而ReentrantLock#lockInterruptibly方法是可以被中断的；
- 在发生异常时Synchronized会自动释放锁（由javac编译时自动实现），而ReentrantLock需要开发者在finally块中显示释放锁；
- ReentrantLock获取锁的形式有多种：如立即返回是否成功的tryLock(),以及等待指定时长的获取，更加灵活；
- Synchronized在特定的情况下对于已经在等待的线程是后来的线程先获得锁（上文有说），而ReentrantLock对于已经在等待的线程一定是先来的线程先获得锁；

&nbsp;

# **五、ThreadLocal**
**ThreadLocal的作用是提供线程内的局部变量，这种变量在线程的生命周期内起作用，减少同一个线程内多个函数或者组件之间一些公共变量的传递的复杂度**

![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20191222163512.png)

https://km.sankuai.com/page/57870121

&nbsp;

# **六、AbstractQueuedSynchronizer（AQS）**
https://lyoga.github.io/post/2019/03-10-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/

- 锁状态：要知道锁是不是被别的线程占有了，这个就是 state 的作用，它为 0 的时候代表没有线程占有锁，可以去争抢这个锁，用 CAS 将 state 设为 1，如果 CAS 成功，说明抢到了锁，这样其他线程就抢不到了，如果锁重入的话，state进行 +1 就可以，解锁就是 -1，直到 state 又变为 0，代表释放锁，所以 lock() 和 unlock() 必须要配对，然后唤醒等待队列中的第一个线程，让其来占有锁
- 线程的阻塞和解除阻塞：AQS 中采用了 LockSupport.park(thread) 来挂起线程，用 unpark 来唤醒线程
- 阻塞队列：因为争抢锁的线程可能很多，但是只能有一个线程拿到锁，其他的线程都必须等待，这个时候就需要一个 queue 来管理这些线程，AQS 用的是一个 FIFO 的队列，就是一个链表，每个 node 都持有后继节点的引用
- AQS 采用了 CLH 锁的变体来实现（CLH锁也是一种基于链表的可扩展、高性能、公平的自旋锁，申请线程只在本地变量上自旋，它不断轮询前驱的状态，如果发现前驱释放了锁就结束自旋）

&nbsp;

# **七、UNSAFE**
https://tech.meituan.com/2019/02/14/talk-about-java-magic-class-unsafe.html

&nbsp;

# **八、LockSupport**
## **8.1 原理**
- LockSupprot是线程的阻塞原语，用来阻塞线程和唤醒线程
- 每个使用LockSupport的线程都会与一个许可关联，如果该许可可用，并且可在线程中使用，则调用park()将会立即返回，否则可能阻塞
- 如果许可尚不可用，则可以调用 unpark 使其可用。但是注意许可不可重入，也就是说只能调用一次park()方法，否则会一直阻塞

&nbsp;

## **8.2 相关方法**

**阻塞线程**

- void park()：阻塞当前线程，如果调用unpark方法或者当前线程被中断，能从park()方法中返回
- void park(Object blocker)：功能同方法1，入参增加一个Object对象，用来记录导致线程阻塞的阻塞对象，方便进行问题排查；
- void parkNanos(long nanos)：阻塞当前线程，最长不超过nanos纳秒，增加了超时返回的特性；
- void parkNanos(Object blocker, long nanos)：功能同方法3，入参增加一个Object对象，用来记录导致线程阻塞的阻塞对象，方便进行问题排查；
- void parkUntil(long deadline)：阻塞当前线程，知道deadline；
- void parkUntil(Object blocker, long deadline)：功能同方法5，入参增加一个Object对象，用来记录导致线程阻塞的阻塞对象，方便进行问题排查；

**唤醒线程**

- void unpark(Thread thread):唤醒处于阻塞状态的指定线程

**实际上LockSupport阻塞和唤醒线程的功能是依赖于sun.misc.Unsafe**

&nbsp;

# **九、ReentrantLock**
**基于AQS实现**

**公平锁和非公平锁只有两处不同：**

- 非公平锁在调用 lock 后，首先就会调用 CAS 进行一次抢锁，如果这个时候恰巧锁没有被占用，那么直接就获取到锁返回了
- 非公平锁在 CAS 失败后，和公平锁一样都会进入到 tryAcquire 方法，在 tryAcquire 方法中，如果发现锁这个时候被释放了（state == 0），非公平锁会直接 CAS 抢锁，但是公平锁会判断等待队列是否有线程处于等待状态，如果有则不去抢锁，乖乖排到后面。
- 公平锁和非公平锁就这两点区别，如果这两次 CAS 都不成功，那么后面非公平锁和公平锁是一样的，都要进入到阻塞队列等待唤醒。

**相对来说，非公平锁会有更好的性能，因为它的吞吐量比较大。当然，非公平锁让获取锁的时间变得更加不确定，可能会导致在阻塞队列中的线程长期处于饥饿状态。**

&nbsp;

# **十、ReentrantReadWriteLock**
**ReadLock 使用了共享模式，WriteLock 使用了独占模式**

**AQS 的精髓在于内部的属性 state：**

- 对于独占模式来说，通常就是 0 代表可获取锁，1 代表锁被别人获取了，重入例外
- 而共享模式下，每个线程都可以对 state 进行加减操作

**也就是说，独占模式和共享模式对于 state 的操作完全不一样，那读写锁 ReentrantReadWriteLock 中是怎么使用 state 的呢？答案是将 state 这个 32 位的 int 值分为高 16 位和低 16位，分别用于共享模式和独占模式**

```
abstract static class Sync extends AbstractQueuedSynchronizer {
    // 下面这块说的就是将 state 一分为二，高 16 位用于共享模式，低16位用于独占模式
    static final int SHARED_SHIFT   = 16;
    static final int SHARED_UNIT    = (1 << SHARED_SHIFT);
    static final int MAX_COUNT      = (1 << SHARED_SHIFT) - 1;
    static final int EXCLUSIVE_MASK = (1 << SHARED_SHIFT) - 1;
    // 取 c 的高 16 位值，代表读锁的获取次数(包括重入)
    static int sharedCount(int c)    { return c >>> SHARED_SHIFT; }
    // 取 c 的低 16 位值，代表写锁的重入次数，因为写锁是独占模式
    static int exclusiveCount(int c) { return c & EXCLUSIVE_MASK; }

    // 这个嵌套类的实例用来记录每个线程持有的读锁数量(读锁重入)
    static final class HoldCounter {
        // 持有的读锁数
        int count = 0;
        // 线程 id
        final long tid = getThreadId(Thread.currentThread());
    }

    // ThreadLocal 的子类
    static final class ThreadLocalHoldCounter
        extends ThreadLocal<HoldCounter> {
        public HoldCounter initialValue() {
            return new HoldCounter();
        }
    }
    /**
      * 组合使用上面两个类，用一个 ThreadLocal 来记录当前线程持有的读锁数量
      */
    private transient ThreadLocalHoldCounter readHolds;

    // 用于缓存，记录"最后一个获取读锁的线程"的读锁重入次数，
    // 所以不管哪个线程获取到读锁后，就把这个值占为已用，这样就不用到 ThreadLocal 中查询 map 了
    // 算不上理论的依据：通常读锁的获取很快就会伴随着释放，
    //   显然，在 获取->释放 读锁这段时间，如果没有其他线程获取读锁的话，此缓存就能帮助提高性能
    private transient HoldCounter cachedHoldCounter;

    // 第一个获取读锁的线程(并且其未释放读锁)，以及它持有的读锁数量
    private transient Thread firstReader = null;
    private transient int firstReaderHoldCount;

    Sync() {
        // 初始化 readHolds 这个 ThreadLocal 属性
        readHolds = new ThreadLocalHoldCounter();
        // 为了保证 readHolds 的内存可见性
        setState(getState()); // ensures visibility of readHolds
    }
    ...
}
```
- state 的高 16 位代表读锁的获取次数，包括重入次数，获取到读锁一次加 1，释放掉读锁一次减 1
- state 的低 16 位代表写锁的获取次数，因为写锁是独占锁，同时只能被一个线程获得，所以它代表重入次数
- 每个线程都需要维护自己的 HoldCounter，记录该线程获取的读锁次数，这样才能知道到底是不是读锁重入，用 ThreadLocal 属性 readHolds 维护
- cachedHoldCounter 有什么用？其实没什么用，但能提示性能。将最后一次获取读锁的线程的 HoldCounter 缓存到这里，这样比使用 ThreadLocal 性能要好一些，因为 ThreadLocal 内部是基于 map 来查询的。但是 cachedHoldCounter 这一个属性毕竟只能缓存一个线程，所以它要起提升性能作用的依据就是：通常读锁的获取紧随着就是该读锁的释放。我这里可能表达不太好，但是大家应该是懂的吧。
- firstReader 和 firstReaderHoldCount 有什么用？其实也没什么用，但是它也能提示性能。将"第一个"获取读锁的线程记录在 firstReader 属性中，这里的第一个不是全局的概念，等这个 firstReader 当前代表的线程释放掉读锁以后，会有后来的线程占用这个属性的。firstReader 和 firstReaderHoldCount 使得在读锁不产生竞争的情况下，记录读锁重入次数非常方便快速
- 如果一个线程使用了 firstReader，那么它就不需要占用 cachedHoldCounter
- 个人认为，读写锁源码中最让初学者头疼的就是这几个用于提升性能的属性了，使得大家看得云里雾里的。主要是因为 ThreadLocal 内部是通过一个 ThreadLocalMap 来操作的，会增加检索时间。而很多场景下，执行 unlock 的线程往往就是刚刚最后一次执行 lock 的线程，中间可能没有其他线程进行 lock。还有就是很多不怎么会发生读锁竞争的场景
- Doug Lea 将持有写锁的线程，去获取读锁的过程称为锁降级（Lock downgrading）。这样，此线程就既持有写锁又持有读锁；但是，锁升级是不可以的。线程持有读锁的话，在没释放的情况下不能去获取写锁，因为会发生死锁(遵循获取写锁，获取读锁再释放写锁的次序，写锁能够降级成为读锁)

![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20191222174405.png)

&nbsp;

# **十一、CountDownLatch**
**CountDownLatch 基于 AQS 的共享模式的使用**
![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20191222165609.png)

**原理：AQS 里面的 state 是一个整数值，这边用一个 int count 参数其实初始化就是设置了这个值，所有调用了 await 方法的等待线程会挂起，然后有其他一些线程会做 state = state - 1 操作，当 state 减到 0 的同时，那个将 state 减为 0 的线程会负责唤醒 所有调用了 await 方法的线程**

&nbsp;

# **十二、CyclicBarrier**
**CyclicBarrier 基于 Condition 来实现**
![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20191222165808.png)
```
public class CyclicBarrier {
    // 我们说了，CyclicBarrier 是可以重复使用的，我们把每次从开始使用到穿过栅栏当做"一代"，或者"一个周期"
    private static class Generation {
        boolean broken = false;
    }

    /** The lock for guarding barrier entry */
    private final ReentrantLock lock = new ReentrantLock();

    // CyclicBarrier 是基于 Condition 的
    // Condition 是“条件”的意思，CyclicBarrier 的等待线程通过 barrier 的“条件”是大家都到了栅栏上
    private final Condition trip = lock.newCondition();

    // 参与的线程数
    private final int parties;

    // 如果设置了这个，代表越过栅栏之前，要执行相应的操作
    private final Runnable barrierCommand;

    // 当前所处的“代”
    private Generation generation = new Generation();

    // 还没有到栅栏的线程数，这个值初始为 parties，然后递减
    // 还没有到栅栏的线程数 = parties - 已经到栅栏的数量
    private int count;

    public CyclicBarrier(int parties, Runnable barrierAction) {
        if (parties <= 0) throw new IllegalArgumentException();
        this.parties = parties;
        this.count = parties;
        this.barrierCommand = barrierAction;
    }

    public CyclicBarrier(int parties) {
        this(parties, null);
    }
}
```

&nbsp;

# **十三、Semaphore**
**Semaphore 基于 AQS 的共享模式的使用**

**原理：**

- 创建 Semaphore 实例的时候，需要一个参数 permits，这个基本上可以确定是设置给 AQS 的 state 的，然后每个线程调用 acquire 的时候，执行 state = state - 1，release 的时候执行 state = state + 1，当然，acquire  的时候，如果 state = 0，说明没有资源了，需要等待其他线程 release
- 拥有公平策略和非公平策略

&nbsp;

# **十四、CAS**
**乐观锁策略**

- CAS比较交换的过程可以通俗的理解为CAS(V,O,N)，包含三个值分别为：V 内存地址存放的实际值；O 预期的值（旧值）；N 更新的新值。当V和O相同时，也就是说旧值和内存中实际的值相同表明该值没有被其他线程更改过，即该旧值O就是目前来说最新的值了，自然而然可以将新值N赋值给V。反之，V和O不相同，表明该值已经被其他线程改过了则该旧值O不是最新版本的值了，所以不能将新值N赋给V，返回V即可。当多个线程使用CAS操作一个变量是，只有一个线程会成功，并成功更新，其余会失败。失败的线程会重新尝试，当然也可以选择挂起线程
- CAS的实现需要硬件指令集的支撑，在JDK1.5后虚拟机才可以使用处理器提供的CMPXCHG指令实现

**问题**

- ABA问题 因为CAS会检查旧值有没有变化，这里存在这样一个有意思的问题。比如一个旧值A变为了成B，然后再变成A，刚好在做CAS时检查发现旧值并没有变化依然为A，但是实际上的确发生了变化。解决方案可以沿袭数据库中常用的乐观锁方式，添加一个版本号可以解决。原来的变化路径A->B->A就变成了1A->2B->3C
- 自旋时间过长：自旋时间过长对性能是很大的消耗；如果JVM能支持处理器提供的pause指令，那么在效率上会有一定的提升

&nbsp;

# **十五、原子操作类**
**自旋+CAS操作保证原子性**

## **1. atomic包提供原子更新基本类型的工具类**

1. AtomicBoolean：以原子更新的方式更新boolean；
2. AtomicInteger：以原子更新的方式更新Integer;
3. AtomicLong：以原子更新的方式更新Long；

这几个类的用法基本一致，这里以AtomicInteger为例总结常用的方法

- addAndGet(int delta) ：以原子方式将输入的数值与实例中原本的值相加，并返回最后的结果；
- incrementAndGet() ：以原子的方式将实例中的原值进行加1操作，并返回最终相加后的结果；
- getAndSet(int newValue)：将实例中的值更新为新值，并返回旧值；
- getAndIncrement()：以原子的方式将实例中的原值加1，返回的是自增前的旧值；

```
public final int getAndIncrement() {
    return unsafe.getAndAddInt(this, valueOffset, 1);
}
```
**可以看出，该方法实际上是调用了unsafe实例的getAndAddInt方法，unsafe实例的获取时通过UnSafe类的静态方法getUnsafe获取**

&nbsp;

## **2. atomic包下提供能原子更新数组中元素的类**

1. AtomicIntegerArray：原子更新整型数组中的元素
2. AtomicLongArray：原子更新长整型数组中的元素
3. AtomicReferenceArray：原子更新引用类型数组中的元素

这几个类的用法一致，就以AtomicIntegerArray来总结下常用的方法：

- addAndGet(int i, int delta)：以原子更新的方式将数组中索引为i的元素与输入值相加；
- getAndIncrement(int i)：以原子更新的方式将数组中索引为i的元素自增加1；
- compareAndSet(int i, int expect, int update)：将数组中索引为i的位置的元素进行更新

&nbsp;

## **3. 如果需要更新对象的某个字段，并在多线程的情况下，能够保证线程安全，atomic同样也提供了相应的原子操作类**

- AtomicIntegeFieldUpdater：原子更新整型字段类
- AtomicLongFieldUpdater：原子更新长整型字段类
- AtomicStampedReference：原子更新引用类型，这种更新方式会带有版本号。而为什么在更新的时候会带有版本号，是为了解决CAS的ABA问题

**要想使用原子更新字段需要两步操作：**

- 原子更新字段类都是抽象类，只能通过静态方法newUpdater来创建一个更新器，并且需要设置想要更新的类和属性
- 更新类的属性必须使用public volatile进行修饰

&nbsp;

# **十六、LongAdder**
**核心思想：分治**

1. 每一步都优先考虑代价相对小的操作。
2. 在开始并发比较低的时候先考虑在base上累加，有竞争失败才去创建cells
3. 如果有了cells之后，先考虑在对应的cell上进行cas操作，有竞争失败才会进入自旋

**要从LongAdder中获取当前累加的总值，就会把base值和所有Cell分段数值加起来返回给你**

**AtomicLong可不可以不要？**   

1. incrementAndGet()有返回值；而 increment() 无返回值，需要再进行一次求和
2. LongAdder 使用了一个 cell 列表去承接并发的 cas，以提升性能，但是 LongAdder 在统计的时候如果有并发更新，可能导致统计的数据有误差；如果用于自增 id 的生成，就不适合使用 LongAdder 了。这个时候使用 AtomicLong 就是一个明智的选择

**为什么用Cell数组而不是long数组？**

扩容的时候，long是栈中存储，可能会发生数据丢失的问题

&nbsp;

# **十七、集合**

## **1.7 HashMap**
![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20191220191013.png)
数据结构：数组 + 链表

- capacity：当前数组容量，始终保持 2^n，可以扩容，扩容后数组大小为当前的 2 倍
- loadFactor：负载因子，默认为 0.75
- threshold：扩容的阈值，等于 capacity * loadFactor

**1.put**

1. 当插入第一个元素的时候，需要先初始化数组大小
2. 如果 key 为 null，最终会将这个 entry 放到 table[0] 中
3. 求key的hash值，找出对应数组下标
4. 遍历一下对应下标处的链表，看是否有重复的 key 已经存在，如果有，直接覆盖，put 方法返回旧值就结束了，不存在重复的 key，将此 entry 添加到链表中

**2.resize**

- 超过最大容量开始扩容，先扩容后插入元素，因为hashCode需要重新计算，而1.8不需要，先插入后扩容
- 容量扩大两倍，将原来数组中的值迁移到新的更大的数组中，由于是双倍扩容，迁移过程中，会将原来 table[i] 中的链表的所有节点，分拆到新的数组的 newTable[i] 和 newTable[i + oldLength] 位置上，但这里需要注意，1.7的resize所有的hashCode需要重新计算一遍，才去计算数组下标
- 采用头插法，链表顺序颠倒

&nbsp;

## **1.8 HashMap**
![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20191221144849.png)
数据结构：数组 + (链表 | 红黑树)

**1.put**

**通过e instanceof TreeNode判断是否进入红黑树操作**

```
static final int hash(Object key) {   //jdk1.8 & jdk1.7
     int h;
     // h = key.hashCode() 为第一步 取hashCode值
     // h ^ (h >>> 16)  为第二步 高位参与运算
     return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
}

static int indexFor(int h, int length) {  //jdk1.7的源码，jdk1.8没有这个方法，但是实现原理一样的
     return h & (length-1);  //第三步 取模运算（&运算效率比%效率高）
}
```

**2. 1.8优化点**

- 由 数组+链表 的结构改为 数组+链表+红黑树，相对于链表，改为红黑树后碰撞元素越多查询效率越高。链表O(n)，红黑树O(logn)
- 在链表元素数量超过8时改为红黑树，少于6时改为链表，中间7不改是避免频繁转换降低性能
- 优化了高位运算的hash算法：h^(h>>>16)，将hashcode无符号右移16位，让高16位和低16位进行异或
- 扩容后，元素要么是在原位置，要么是在原位置再移动2次幂的位置，且链表顺序不变
- 不需要重新计算hash，只需要根据原来hash值新增的bit是1还是0分别放进两个链表lo和hi（非红黑树的情况）里，0的话索引没变，1的话索引变为原索引加原来的数组长度
- 因为用的尾插法所以新数组链表不会倒置，多线程下不会出现死循环

&nbsp;

## **1.7 ConcurrentHashMap**
![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20191221145631.png)
**ConcurrentHashMap 是一个 Segment 数组，Segment 通过继承 ReentrantLock 来进行加锁，所以每次需要加锁的操作锁住的是一个 segment，这样只要保证每个 Segment 是线程安全的，也就实现了全局的线程安全**

- concurrencyLevel：并行级别、并发数、Segment 数，怎么翻译不重要，理解它。默认是 16，也就是说 ConcurrentHashMap 有 16 个 Segments，所以理论上，这个时候，最多可以同时支持 16 个线程并发写，只要它们的操作分别分布在不同的 Segment 上。这个值可以在初始化的时候设置为其他值，但是一旦初始化以后，它是不可以扩容的
- Segment 数组长度为 16，不可以扩容
- Segment[i] 的默认大小为 2，负载因子是 0.75，得出初始阈值为 1.5，也就是以后插入第一个元素不会触发扩容，插入第二个会进行第一次扩容
- 这里初始化了 segment[0]，其他位置还是 null，至于为什么要初始化 segment[0]，因为后面初始化其他segment需要用到capacity、loadFactor
- 当前 segmentShift 的值为 32 - 4 = 28，segmentMask 为 16 - 1 = 15，姑且把它们简单翻译为移位数和掩码

**1.put**

1. 计算key的hash值，根据 hash 值找到 Segment 数组中的位置 j
```
int j = (hash >>> segmentShift) & segmentMask;
```
2. 如果Segment未初始化，就初始化
  1. 使用当前 segment[0] 处的数组长度和负载因子来初始化 segment[k]
  2. 使用 while 循环，内部用 CAS，当前线程成功设值或其他线程成功设值后，退出
3. 插入Segment中
  1. 先Segment.lock，再利用 hash 值，求应该放置的数组下标
  2. 覆盖旧值，或者新建节点，或者扩容

**2.resize**

1. 新建2倍容量数组，遍历原数组，老套路，将原数组位置 i 处的链表拆分到 新数组位置 i 和 i+oldCap 两个位置
2. for 循环会找到一个 lastRun 节点，这个节点之后的所有元素是将要放到一起的
3. 将 lastRun 及其之后的所有节点组成的这个链表放到 lastIdx 这个位置
4. 处理 lastRun 之前的节点，这些节点可能分配在另一个链表中，也可能分配到上面的那个链表中

**lastRun这么做是为了减少节点clone，根据统计，如果使用默认的阈值，大约只有 1/6 的节点需要克隆**

**3.size**

- 1.7在统计size的时候，会先用加锁的情况下统计2次，然后比较差异，如果有差异，就会获取所有的Segment锁，再去统计
- for循环把所有segment.lock，统计累加完之后，再for循环把所有segment.unlock

&nbsp;

## **1.8 ConcurrentHashMap**
![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20191221152211.png)

**1.关键属性**

1. table volatile Node<K,V>[] table://装载Node的数组，作为ConcurrentHashMap的数据容器，采用懒加载的方式，直到第一次插入数据的时候才会进行初始化操作，数组的大小总是为2的幂次方。
2. nextTable volatile Node<K,V>[] nextTable; //扩容时使用，平时为null，只有在扩容的时候才为非null
3. sizeCtl volatile int sizeCtl; 该属性用来控制table数组的大小，根据是否初始化和是否正在扩容有几种情况：
  1. 当值为负数时：如果为-1表示正在初始化，如果为-N则表示当前正有N-1个线程进行扩容操作；
  2. 当值为正数时：如果当前数组为null的话表示table在初始化过程中，sizeCtl表示为需要新建数组的长度；若已经初始化了，表示当前数据容器（table数组）可用容量也可以理解成临界值（插入节点数超过了该临界值就需要扩容），具体指为数组的长度n * loadFactor；当值为0时，即数组长度为默认初始值。
4. sun.misc.Unsafe U 在ConcurrentHashMapde的实现中可以看到大量的U.compareAndSwapXXXX的方法去修改ConcurrentHashMap的一些属性。这些方法实际上是利用了CAS算法保证了线程安全性，这是一种乐观策略，假设每一次操作都不会产生冲突，当且仅当冲突发生的时候再去尝试。而CAS操作依赖于现代处理器指令集，通过底层CMPXCHG指令实现。
  - CAS(V,O,N)核心思想为：若当前变量实际值V与期望的旧值O相同，则表明该变量没被其他线程进行修改，因此可以安全的将新值N赋值给变量；若当前变量实际值V与期望的旧值O不相同，则表明该变量已经被其他线程做了处理，此时将新值N赋给变量操作就是不安全的，在进行重试。而在大量的同步组件和并发容器的实现中使用CAS是通过sun.misc.Unsafe类实现的，该类提供了一些可以直接操控内存和线程的底层操作，可以理解为java中的“指针”。该成员变量的获取是在静态代码块中

**2.put**
1. ConcurrentHashMap的默认构造函数为空， 真正的初始化是在put的时候

**链表转红黑树的话，如果数组长度小于 64 的时候，其实也就是 32 或者 16 或者更小的时候，会进行数组扩容**

**3.resize**
核心是transfer方法，通过给每个线程分配桶区间，避免线程间的争用，通过为每个桶节点加锁，避免 putVal 方法导致数据不一致。同时，在扩容的时候，也会将链表拆成两份，这点和 HashMap 的 resize 方法类似。

**4.size**

**1.8认为size是一个瞬间的状态，所以它将每次修改的计数放在了ConcurrentHashMap的局部变量中，调用size的时候，直接去获取这个计数**

**5. 1.8优化点**
- 抛弃了Segment分段锁机制，利用CAS + synchronized(头结点)来保证并发更新的安全，底层依然采用数组+链表+红黑树的存储结构
- basecount 记录元素数量，通过CAS更新
- counterCells 记录元素变化个数，cas操作basecount失败时使用
- 扩容的元素复制可并行进行
- 设计了MOVED状态 当resize的中过程中 线程2还在put数据，线程2会帮助resize

&nbsp;

## **CopyOnWriteArrayList**
**设计思想：写时复制的思想来通过延时更新的策略来实现数据的最终一致性，并且能够保证读线程间不阻塞**

**实现原理**
```
// 该数组引用是被volatile修饰，注意这里仅仅是修饰的是数组引用
private transient volatile Object[] array;
```
 **add**

- 采用ReentrantLock，保证同一时刻只有一个写线程正在进行数组的复制，否则的话内存中会有多份被复制的数据；
- 前面说过数组引用是volatile修饰的，因此将旧的数组引用指向新的数组，根据volatile的happens-before规则，写线程对数组引用的修改对读线程是可见的。
- 由于在写数据的时候，是在新的数组中插入数据的，从而保证读写实在两个不同的数据容器中进行操作。

**COW VS 读写锁**
**相同点：**

1. 两者都是通过读写分离的思想实现
2. 读线程间是互不阻塞的

**不同点：**

1. 对读线程而言，为了实现数据实时性，在写锁被获取后，读线程会等待或者当读锁被获取后，写线程会等待，从而解决“脏读”等问题。也就是说如果使用读写锁依然会出现读线程阻塞等待的情况
2. 而COW则完全放开了牺牲数据实时性而保证数据最终一致性，即读线程对数据的更新是延时感知的，因此读线程不会存在等待的情况

**COW缺点**

1. 内存占用问题：因为CopyOnWrite的写时复制机制，所以在进行写操作的时候，内存里会同时驻扎两个对 象的内存，旧的对象和新写入的对象（注意:在复制的时候只是复制容器里的引用，只是在写的时候会创建新对象添加到新容器里，而旧容器的对象还在使用，所以有两份对象内存）。如果这些对象占用的内存比较大，比如说200M左右，那么再写入100M数据进去，内存就会占用300M，那么这个时候很有可能造成频繁的minor GC和major GC
2. 数据一致性问题：CopyOnWrite容器只能保证数据的最终一致性，不能保证数据的实时一致性。所以如果你希望写入的的数据，马上能读到，请不要使用CopyOnWrite容器

## **ConcurrentLinkedQueue**

- ConcurrentLinkedQueue的底层使用单向链表数据结构来保存队列元素，每个元素被包装成一个 Node 节点。队列是靠头、尾节点来维护的，创建队列时头、尾节点指向－个 item 为 null 的哨兵节点。第一次执行 peek 或者first 操作时会把 head 指向第一个真正的队 列元素。由于使用非阻塞 CAS 算法，没有加锁，所以在计算 size 时有可能进行了offer、poll或者remove操作，导致计算的元素个数不精确，所以在井发情况下 size 函数不是很 有用。
- 入队、出队都是操作使用 volatile 修饰的 tail、head 节点，要保证在多线程下出入队线程安全，只需要保证这两个 Node 操作的可见性和原子性即可。由于 volatile 本身可以保证可见性，所以只需要保证对两个变量操作的原子性即可。

&nbsp;

# **十八、线程池**
## **1.为何使用线程池**
1. 降低资源消耗
2. 提升系统响应速度
3. 提高线程的可管理性

&nbsp;

## **2.工作原理**
![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20191221163147.png)
**核心线程数 - 阻塞队列 - 最大线程数 - 拒绝策略**

&nbsp;

## **3.线程池创建**
```
ThreadPoolExecutor(int corePoolSize,
                              int maximumPoolSize,
                              long keepAliveTime,
                              TimeUnit unit,
                              BlockingQueue<Runnable> workQueue,
                              ThreadFactory threadFactory,
                              RejectedExecutionHandler handler)
```
- corePoolSize：表示核心线程池的大小
  - 当提交一个任务时，如果当前核心线程池的线程个数没有达到corePoolSize，则会创建新的线程来执行所提交的任务，**即使当前核心线程池有空闲的线程**
  - 如果当前核心线程池的线程个数已经达到了corePoolSize，则不再重新创建线程
  - 如果调用了prestartCoreThread()或者 prestartAllCoreThreads()，线程池创建的时候所有的核心线程都会被创建并且启动。
- maximumPoolSize：表示线程池能创建线程的最大个数
  - 如果当阻塞队列已满时，并且当前线程池线程个数没有超过maximumPoolSize的话，就会创建新的线程来执行任务
- keepAliveTime：空闲线程存活时间
  - 如果当前线程池的线程个数已经超过了corePoolSize，并且线程空闲时间超过了keepAliveTime的话，就会将这些空闲线程销毁，这样可以尽可能降低系统资源消耗
  - 也可以通过调用 allowCoreThreadTimeOut(true)使核心线程数内的线程也可以被回收
- unit：时间单位，为keepAliveTime指定时间单位
- workQueue：阻塞队列。用于保存任务的阻塞队列
  - ArrayBlockingQueue, LinkedBlockingQueue, SynchronousQueue, PriorityBlockingQueue
- threadFactory：创建线程的工程类，通常，我们可以通过它将我们的线程的名字设置得比较可读一些，如 Message-Thread-1， Message-Thread-2 类似这样
- handler：饱和策略，当线程池的阻塞队列已满和指定的线程都已经开启，说明当前线程池已经处于饱和状态了，那么就需要采用一种策略来处理这种情况
  - AbortPolicy（终止策略）： 直接拒绝所提交的任务，并抛出RejectedExecutionException异常；
  - CallerRunsPolicy（调用者运行策略）：当触发拒绝策略时，只要线程池没有关闭，就由提交任务的当前线程处理；不允许失败的、对性能要求不高、并发量较小的场景下使用
  - DiscardPolicy（丢弃策略）：不处理直接丢弃掉任务；
  - DiscardOldestPolicy（弃老策略）：丢弃掉阻塞队列中存放时间最久的任务，执行当前任务

&nbsp;

## **4.线程池状态**
```
private static final int RUNNING    = -1 << COUNT_BITS;
private static final int SHUTDOWN   =  0 << COUNT_BITS;
private static final int STOP       =  1 << COUNT_BITS;
private static final int TIDYING    =  2 << COUNT_BITS;
private static final int TERMINATED =  3 << COUNT_BITS;
```

**采用一个 32 位的整数来存放线程池的状态和当前池中的线程数，其中高 3 位用于存放线程池状态，低 29 位表示线程数**

- RUNNING：这个没什么好说的，这是最正常的状态：接受新的任务，处理等待队列中的任务
- SHUTDOWN：不接受新的任务提交，但是会继续处理等待队列中的任务
- STOP：不接受新的任务提交，不再处理等待队列中的任务，中断正在执行任务的线程
- TIDYING：所有的任务都销毁了，workCount 为 0。线程池的状态在转换为 TIDYING 状态时，会执行钩子方法 terminated()
- TERMINATED：terminated() 方法结束后，线程池的状态就会变成这个

**状态转换**

- RUNNING -> SHUTDOWN：当调用了 shutdown() 后，会发生这个状态转换，这也是最重要的
- (RUNNING or SHUTDOWN) -> STOP：当调用 shutdownNow() 后，会发生这个状态转换，这下要清楚 shutDown() 和 shutDownNow() 的区别了
- SHUTDOWN -> TIDYING：当任务队列和线程池都清空后，会由 SHUTDOWN 转换为 TIDYING
- STOP -> TIDYING：当任务队列清空后，发生这个转换
- TIDYING -> TERMINATED：这个前面说了，当 terminated() 方法结束后

**任务是 Runnable（内部变量名叫 task 或 command），线程是 Worker**

&nbsp;

## **5.任务执行过程中发生异常怎么处理？**
**如果某个任务执行出现异常，那么执行任务的线程会被关闭，而不是继续接收其他任务。然后会启动一个新的线程来代替它**

&nbsp;

## **6.什么时候会执行拒绝策略？**
- workers 的数量达到了 corePoolSize（任务此时需要进入任务队列），任务入队成功，与此同时线程池被关闭了，而且关闭线程池并没有将这个任务出队，那么执行拒绝策略。这里说的是非常边界的问题，入队和关闭线程池并发执行，读者仔细看看 execute 方法是怎么进到第一个 reject(command) 里面的
- workers 的数量大于等于 corePoolSize，将任务加入到任务队列，可是队列满了，任务入队失败，那么准备开启新的线程，可是线程数已经达到 maximumPoolSize，那么执行拒绝策略

&nbsp;

## **7.如何合理配置线程池参数**
**考虑因素**

- 任务的性质：CPU密集型任务，IO密集型任务和混合型任务
- 任务的优先级：高，中和低
- 任务的执行时间：长，中和短
- 任务的依赖性：是否依赖其他系统资源，如数据库连接

**场景**

- 任务性质不同的任务可以用不同规模的线程池分开处理
- CPU密集型任务配置尽可能少的线程数量，如配置Ncpu+1个线程的线程池
- IO密集型任务则由于需要等待IO操作，线程并不是一直在执行任务，则配置尽可能多的线程，如2xNcpu
- 混合型的任务，如果可以拆分，则将其拆分成一个CPU密集型任务和一个IO密集型任务，只要这两个任务执行的时间相差不是太大，那么分解后执行的吞吐率要高于串行执行的吞吐率，如果这两个任务执行时间相差太大，则没必要进行分解；我们可以通过Runtime.getRuntime().availableProcessors()方法获得当前设备的CPU个数

最佳线程数目 = （（线程等待时间+线程CPU时间）/ 线程CPU时间 ）* CPU数目
下面举个例子：比如平均每个线程CPU运行时间为0.5s，而线程等待时间（非CPU运行时间，比如IO）为1.5s，CPU核心数为8，那么根据上面这个公式估算得到：((0.5+1.5)/0.5) * 8=32
这个公式进一步转化为：最佳线程数目 = （线程等待时间与线程CPU时间之比 + 1）* CPU数目


&nbsp;

## **8.Executors**

1. newFixedThreadPool
  - 生成一个固定大小的线程池，最大线程数设置为与核心线程数相等，此时 keepAliveTime 设置为 0（因为这里它是没用的，即使不为 0，线程池默认也不会回收 corePoolSize 内的线程），任务队列采用 LinkedBlockingQueue，无界队列
2. newSingleThreadExecutor
  - 生成只有一个线程的固定线程池，corePoolSize = maximumPoolSize
3. newCachedThreadPool
  - 生成一个需要的时候就创建新的线程，同时可以复用之前创建的线程（如果这个线程当前没有任务）的线程池
  - 核心线程数为 0，最大线程数为 Integer.MAX_VALUE，keepAliveTime 为 60 秒，任务队列采用 SynchronousQueue
4. newScheduledThreadPool
  - 创建一个定长线程池，支持定时及周期性任务执行
5. newWorkStealingPool
  - newWorkStealingPool适合使用在很耗时的操作，但是newWorkStealingPool不是ThreadPoolExecutor的扩展，它是新的线程池类ForkJoinPool的扩展，但是都是在统一的一个Executors类中实现，由于能够合理的使用CPU进行对任务操作（并行操作），所以适合使用在很耗时的任务中

![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20191222110826.png)

&nbsp;

## **9.ForkJoinPool**

- 实现更细粒度的并行性
- 更好地处理递归计算问题
- 更好地并行处理计算密集性任务

**实现原理**

- 用有限的线程（通常为核心线程数）并为每个线程一个工作队列
- 当线程处理完它的任务队列时，可以从另外线程工作队列中偷取一个任务来执行

![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20191222111358.png)

**1.8 自动化并行**

- JAVA 8提供了一个静态的ForkJoinPool，默认线程数等于处理器核心数
- Stream中并行的API自动使用静态的ForkJoinPool来实现自动并行化

**注意事项**

- 子任务规模
  - 规模过小，产生过量的任务对象，频繁GC
  - 规模过大无法充分发挥并发
  - Java Doc建议100~10000个计算步骤
- 不要调用阻塞方法

&nbsp;

## **10.BlockingQueue**
**BlockingQueue 对插入操作、移除操作、获取元素操作提供了四种不同的方法用于不同的场景中使用：**

1. 抛出异常
2. 返回特殊值（null 或 true/false，取决于具体的操作）
3. 阻塞等待此操作，直到这个操作成功
4. 阻塞等待此操作，直到成功或者超时指定时间
![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20191222114844.png)

### **实现**
**ArrayBlockingQueue（有界队列）**
```
// 用于存放元素的数组
final Object[] items;
// 下一次读取操作的位置
int takeIndex;
// 下一次写入操作的位置
int putIndex;
// 队列中的元素数量
int count;

// 以下几个就是控制并发用的同步器
final ReentrantLock lock;
private final Condition notEmpty;
private final Condition notFull;
```
![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20191222141019.png)

- ArrayBlockingQueue 实现并发同步的原理就是，读操作和写操作都需要获取到 AQS 独占锁才能进行操作
- 如果队列为空，这个时候读操作的线程进入到读线程队列排队，等待写线程写入新的元素，然后唤醒读线程队列的第一个等待线程
- 如果队列已满，这个时候写操作的线程进入到写线程队列排队，等待读线程将队列元素移除腾出空间，然后唤醒写线程队列的第一个等待线程

&nbsp;

**LinkedBlockingQueue（无界队列，也可看做有界，长度Integer.MAX_VALUE）**
```
// 队列容量
private final int capacity;

// 队列中的元素数量
private final AtomicInteger count = new AtomicInteger(0);

// 队头
private transient Node<E> head;

// 队尾
private transient Node<E> last;

// take, poll, peek 等读操作的方法需要获取到这个锁
private final ReentrantLock takeLock = new ReentrantLock();

// 如果读操作的时候队列是空的，那么等待 notEmpty 条件
private final Condition notEmpty = takeLock.newCondition();

// put, offer 等写操作的方法需要获取到这个锁
private final ReentrantLock putLock = new ReentrantLock();

// 如果写操作的时候队列是满的，那么等待 notFull 条件
private final Condition notFull = putLock.newCondition();
```

- takeLock 和 notEmpty 怎么搭配：如果要获取（take）一个元素，需要获取 takeLock 锁，但是获取了锁还不够，如果队列此时为空，还需要队列不为空（notEmpty）这个条件（Condition）
- putLock 需要和 notFull 搭配：如果要插入（put）一个元素，需要获取 putLock 锁，但是获取了锁还不够，如果队列此时已满，还需要队列不是满的（notFull）这个条件（Condition）

![](https://lyoga-1257336739.cos.ap-beijing.myqcloud.com/20191222141641.png)

&nbsp;

**SynchronousQueue**

- 实际上它不是一个真正的队列，因为它不会为队列中元素维护存储空间。与其他队列不同的是，它维护一组线程，这些线程在等待着把元素加入或移出队列
- SynchronousQueue没有存储功能，因此put和take会一直阻塞，直到有另一个线程已经准备好参与到交付过程中。仅当有足够多的消费者，并且总是有一个消费者准备好获取交付的工作时，才适合使用同步队列
- 如果应用程序确实需要比较大的工作队列容量，而又想避免无界工作队列可能导致的问题，不妨考虑SynchronousQueue,SynchronousQueue实现上并不使用缓存空间
- 使用SynchronousQueue的目的就是保证“对于提交的任务，如果有空闲线程，则使用空闲线程来处理；否则新建一个线程来处理任务”

&nbsp;
     
**PriorityBlockingQueue**
**带排序的 BlockingQueue 实现，其并发控制采用的是 ReentrantLock，队列为无界队列（ArrayBlockingQueue 是有界队列，LinkedBlockingQueue 也可以通过在构造函数中传入 capacity 指定队列最大的容量，但是 PriorityBlockingQueue 只能指定初始的队列大小，后面插入元素的时候，如果空间不够的话会自动扩容）**
```
// 构造方法中，如果不指定大小的话，默认大小为 11
private static final int DEFAULT_INITIAL_CAPACITY = 11;
// 数组的最大容量
private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8;

// 这个就是存放数据的数组
private transient Object[] queue;

// 队列当前大小
private transient int size;

// 大小比较器，如果按照自然序排序，那么此属性可设置为 null
private transient Comparator<? super E> comparator;

// 并发控制所用的锁，所有的 public 且涉及到线程安全的方法，都必须先获取到这个锁
private final ReentrantLock lock;

// 这个很好理解，其实例由上面的 lock 属性创建
private final Condition notEmpty;

// 这个也是用于锁，用于数组扩容的时候，需要先获取到这个锁，才能进行扩容操作
// 其使用 CAS 操作
private transient volatile int allocationSpinLock;

// 用于序列化和反序列化的时候用，对于 PriorityBlockingQueue 我们应该比较少使用到序列化
private PriorityQueue q;
```
**PriorityBlockingQueue 使用了基于数组的二叉堆来存放元素，所有的 public 方法采用同一个 lock 进行并发控制**

&nbsp;

**总结**

- ArrayBlockingQueue 底层是数组，有界队列，如果我们要使用生产者-消费者模式，这是非常好的选择
- LinkedBlockingQueue 底层是链表，可以当做无界和有界队列来使用，所以大家不要以为它就是无界队列
- SynchronousQueue 本身不带有空间来存储任何元素，使用上可以选择公平模式和非公平模式
- PriorityBlockingQueue 是无界队列，基于数组，数据结构为二叉堆，数组第一个也是树的根节点总是最小值
                     
       
         
            
              
            
              
